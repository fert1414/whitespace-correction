{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83155421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040feca",
   "metadata": {},
   "source": [
    "Функционал для загрузки и очистки текста от лишних символов.\n",
    "\n",
    "1. Нормализация Unicode\n",
    "2. Удаление управляющих символов\n",
    "3. Удаление эмодзи и пиктограмм\n",
    "4. Приведение любой пробельной последовательности к одному пробелу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f40c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Управляющие символы: все категории Unicode, начинающиеся на 'C' (Cc, Cf, Cs, Co, Cn)\n",
    "RE_CONTROL = re.compile(r'[\\x00-\\x1F\\x7F]')\n",
    "\n",
    "# Эмодзи и пиктографика: покрывает основные блоки Unicode, ZWJ, вариационные селекторы и пр.\n",
    "RE_EMOJI = re.compile(\n",
    "    '['                    \n",
    "    '\\U0001F600-\\U0001F64F'\n",
    "    '\\U0001F300-\\U0001F5FF'\n",
    "    '\\U0001F680-\\U0001F6FF'\n",
    "    '\\U0001F1E0-\\U0001F1FF'\n",
    "    '\\U00002500-\\U00002BEF'\n",
    "    '\\U00002702-\\U000027B0'\n",
    "    '\\U000024C2-\\U0001F251'\n",
    "    '\\U0001F926-\\U0001F937'\n",
    "    '\\U00010000-\\U0010FFFF'\n",
    "    '\\u2640-\\u2642'\n",
    "    '\\u2600-\\u2B55'\n",
    "    '\\u200d'\n",
    "    '\\u23cf'\n",
    "    '\\u23e9'\n",
    "    '\\u231a'\n",
    "    '\\ufe0f'\n",
    "    '\\u3030'\n",
    "    ']+',\n",
    "    flags = re.UNICODE\n",
    ")\n",
    "\n",
    "# Все виды пробелов в один:\n",
    "RE_SPACES = re.compile(r'\\s+')\n",
    "\n",
    "# Стандартная нормализация Unicode\n",
    "def normalize_unicode(string: str, form: str = 'NFC') -> str:\n",
    "    return unicodedata.normalize(form, string)\n",
    "\n",
    "# Удаляем ASCII-контроли через диапазон, остальные — через категорию\n",
    "def remove_control_chars(string: str) -> str:\n",
    "    string = RE_CONTROL.sub('', string)\n",
    "    string = ''.join(ch for ch in string if unicodedata.category(ch)[0] != 'C')\n",
    "    return string\n",
    "\n",
    "# Удаляем эмодзи\n",
    "def remove_emojis(string: str) -> str:\n",
    "    return RE_EMOJI.sub('', string)\n",
    "\n",
    "# Приводим все виды пробелов к обычному пробелу, режем по краям\n",
    "def normalize_spaces(string: str) -> str:\n",
    "    string = RE_SPACES.sub(' ', string)\n",
    "    return string.strip()\n",
    "\n",
    "def clean_line(string: str) -> str:\n",
    "    string = normalize_unicode(string, 'NFC')         # Unicode нормализация\n",
    "    string = remove_control_chars(string)             # Удаление control chars\n",
    "    string = remove_emojis(string)                    # Удаление эмодзи\n",
    "    string = normalize_spaces(string)                 # Нормализация пробелов\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd393b00",
   "metadata": {},
   "source": [
    "Функционал для загрузки текста\n",
    "\n",
    "1. Загружаем и очищаем текст для тренировки модели и сохраняем в один файл\n",
    "2. Загружаем файл очищенного текста и преобразовываем его в массив строк для обучения модели\n",
    "3. Загружаем тестовые текстовые данные в формат pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6777f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем тексты с необработанными данными, очищаем и сохраняем в один файл\n",
    "def load_and_clean_txt(paths: str | Path, out_path: str | Path, encoding: str = 'utf-8', newline: str = '\\n'):\n",
    "\n",
    "    file_out_path = Path(out_path)\n",
    "\n",
    "    with file_out_path.open('w', encoding = encoding, newline = newline) as out_file:\n",
    "        for path in paths:\n",
    "            file_path = Path(path)\n",
    "            with file_path.open('r', encoding = encoding, errors = 'replace') as file:\n",
    "                for line in file:\n",
    "                    cleaned = clean_line(line)\n",
    "                    if cleaned:\n",
    "                        out_file.write(cleaned + '\\n')\n",
    "\n",
    "# Загружаем очищенные тексты для, преобразуя его в массив строк\n",
    "def load_cleaned_text(path: str | Path, encoding: str = 'utf-8'):\n",
    "    file_path = Path(path)\n",
    "    text = file_path.read_text(encoding = encoding)\n",
    "    lines = text.splitlines()\n",
    "    return lines\n",
    "\n",
    "# Загружаем тексты для теста в формате pd.DataFrame\n",
    "def load_test_text(path: str | Path, encoding: str = 'utf-8'):\n",
    "    rows_data = []\n",
    "\n",
    "    file_path = Path(path)\n",
    "    with file_path.open('r', encoding = encoding) as file:\n",
    "        first_string = True\n",
    "\n",
    "        for line in file:\n",
    "            if first_string:\n",
    "                first_string = False\n",
    "                continue\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            line = line.rstrip(\"\\n\\r\")\n",
    "            parts = line.split(\",\", 1)\n",
    "\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "\n",
    "            id, text_no_spaces = parts[0].strip(), parts[1].strip()\n",
    "            rows_data.append((int(id), text_no_spaces))\n",
    "\n",
    "    return pd.DataFrame(rows_data, columns = ['id', 'text_no_spaces'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a29d23",
   "metadata": {},
   "source": [
    "Загружаем исходные тексты и обрабатываем их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c58023",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_paths = ['./raw_datasets/articles.txt', \n",
    "                  './raw_datasets/books-A.txt', \n",
    "                  './raw_datasets/poems.txt', \n",
    "                  './raw_datasets/books-B.txt',\n",
    "                  './raw_datasets/fanfiction.txt']\n",
    "\n",
    "load_and_clean_txt(paths = raw_text_paths, out_path = 'processed_text.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141254a",
   "metadata": {},
   "source": [
    "Реализация модели Char-level n-gram LM с beam search\n",
    "\n",
    "1. Английские буквы и цифры заменяются на специальные токены\n",
    "2. Параметры модели: \n",
    "    \n",
    "    · n - количество предыдущих символов для\n",
    "    \n",
    "    · add_k - параметр сглаживания, для избежания нулевых вероятностей у редких n-gramm\n",
    "    \n",
    "    · lower - приводить ли текст к нижнему регистру\n",
    "3. Функция _prep() превращает строку в последовательность символов-токенов с нормализацией (буквы → ENG, цифры → DIG)\n",
    "4. Функция fit() строит разреженную условную частотную таблицу «контекст → распределение по следующим символам» и создает словарь символов\n",
    "5. Функция logprob_next() рассчитывает вероятность следующего символа после контекста с использованием сглаживания\n",
    "6. Функция segment_with_beam() реализует beam search учитывая вероятности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENG = \"<ENG>\"\n",
    "DIG = \"<DIG>\"\n",
    "\n",
    "# Нормализация символов для LM: латиница и цифры становятся классами\n",
    "def to_lm_token(char: str) -> str:\n",
    "    oc = char\n",
    "    if oc == \" \":\n",
    "        return \" \"\n",
    "    if (\"A\" <= oc <= \"Z\") or (\"a\" <= oc <= \"z\"):\n",
    "        return ENG\n",
    "    if \"0\" <= oc <= \"9\":\n",
    "        return DIG\n",
    "    return oc\n",
    "\n",
    "class CharNGramLM:\n",
    "    def __init__(self, n: int = 5, add_k: float = 0.1, lower: bool = True):\n",
    "        self.n = n\n",
    "        self.add_k = add_k\n",
    "        self.lower = lower\n",
    "\n",
    "        # context_counts: словарь {контекст(tuple из n-1 токенов) -> Counter следующих символов}\n",
    "        self.context_counts = defaultdict(Counter)\n",
    "        # vocab: множество всех наблюдавшихся символов-выходов (включая спецтокены)\n",
    "        self.vocab = set()\n",
    "        # Спецтокены начала/конца последовательности\n",
    "        self.BOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "\n",
    "    def _prep(self, text: str) -> List[str]:\n",
    "        s = text.lower() if self.lower else text\n",
    "        # для LM заменяем англ. буквы и цифры на классы\n",
    "        return [to_lm_token(c) for c in s]\n",
    "\n",
    "    def fit(self, corpus: List[str]):\n",
    "        for line in corpus:\n",
    "            # Добавляем (n-1) BOS в начало и EOS в конец\n",
    "            toks = [self.BOS] * (self.n - 1) + self._prep(line) + [self.EOS]\n",
    "            for i in range(self.n - 1, len(toks)):\n",
    "                # Контекст — окно из n-1 предыдущих токенов\n",
    "                ctx = tuple(toks[i - (self.n - 1):i])\n",
    "                # Следующий токен после контекста   \n",
    "                nxt = toks[i]\n",
    "                # Увеличиваем счётчик для пары (контекст -> следующий токен)\n",
    "                self.context_counts[ctx][nxt] += 1\n",
    "                # Добавляем наблюдавшийся токен в словарь\n",
    "                self.vocab.add(nxt)\n",
    "        self.vocab.update([self.BOS, self.EOS])\n",
    "        # Размер словаря (нужен для знаменателя при add-k)\n",
    "        self.V = len(self.vocab)\n",
    "\n",
    "    def logprob_next(self, context, nxt: str) -> float:\n",
    "        # Приводим переданный контекст к длине n-1:\n",
    "        # - если контекст короче, добиваем слева BOS\n",
    "        # - если длиннее, берём правые n-1 токенов\n",
    "        if len(context) < self.n - 1:\n",
    "            context = (self.BOS,) * (self.n - 1 - len(context)) + tuple(context)\n",
    "        else:\n",
    "            context = tuple(context[-(self.n - 1):])\n",
    "\n",
    "        # Получаем счётчики для этого контекста\n",
    "        cnts = self.context_counts.get(context)\n",
    "\n",
    "\n",
    "        if cnts is None:\n",
    "            # P(x|контекст_невиден) = k / (k*V) = 1/V\n",
    "            return math.log(self.add_k / (self.add_k * self.V))\n",
    "        \n",
    "        num = cnts.get(nxt, 0) + self.add_k # сглаженный числитель\n",
    "        den = sum(cnts.values()) + self.add_k * self.V # сглаженный знаменатель\n",
    "\n",
    "        # Возвращаем логарифм условной вероятности следующего символа\n",
    "        return math.log(num / den)\n",
    "\n",
    "def segment_with_beam(text: str,\n",
    "                      lm: CharNGramLM,\n",
    "                      beam_size: int = 32,\n",
    "                      max_len: int = 100000,\n",
    "                      space_penalty: float = 0.2,\n",
    "                      min_word_len: int = 1,\n",
    "                      max_consec_spaces: int = 1) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Возвращает (текст с пробелами, позиции пробелов относительно исходной строки без пробелов).\n",
    "    \"\"\"\n",
    "\n",
    "    # Элемент beam — это кортеж состояния гипотезы:\n",
    "    # (lp, out_chars, lwlen, ctx, srun, spos), где:\n",
    "    # - lp: накопленный логарифм вероятности гипотезы (чем больше, тем лучше);\n",
    "    # - out_chars: уже собранные выходные символы (список строк);\n",
    "    # - lwlen: длина последнего текущего слова (счётчик символов с момента последнего пробела);\n",
    "    # - ctx: текущий контекст для LM (tuple из n-1 последних токенов LM);\n",
    "    # - srun: сколько пробелов подряд уже поставлено (для ограничения max_consec_spaces);\n",
    "    # - spos: список позиций, где мы вставили пробелы относительно индексов входного текста.\n",
    "    beam = [ (0.0, [], 0, tuple([lm.BOS] * (lm.n - 1)), 0, []) ]\n",
    "\n",
    "    def extend_with_symbol(hyp, real_ch: str, is_space: bool):\n",
    "        # Расширяет гипотезу добавлением либо пробела, либо реального символа входа\n",
    "        lp, out_chars, lwlen, ctx, srun, spos = hyp\n",
    "\n",
    "        lm_sym = \" \" if is_space else to_lm_token(real_ch)\n",
    "        # Добавляем лог-вероятность символа по LM в текущем контексте\n",
    "        new_lp = lp + lm.logprob_next(ctx, lm_sym)\n",
    "         # Обновляем контекст LM: сдвигаем окно и добавляем текущий символ LM\n",
    "        new_ctx = (ctx + (lm_sym,))[-(lm.n - 1):]\n",
    "\n",
    "        if is_space:\n",
    "            new_srun = srun + 1\n",
    "            new_lwlen = 0\n",
    "            new_out = out_chars + [\" \"]\n",
    "        else:\n",
    "            new_srun = 0\n",
    "            new_lwlen = lwlen + 1\n",
    "            new_out = out_chars + [real_ch]\n",
    "        return (new_lp, new_out, new_lwlen, new_ctx, new_srun, spos)\n",
    "\n",
    "    for i, ch in enumerate(text):\n",
    "        new_beam = []\n",
    "        for hyp in beam:\n",
    "            lp, out_chars, lwlen, ctx, srun, spos = hyp\n",
    "\n",
    "            # Без пробела: просто добавить текущий символ\n",
    "            cont = extend_with_symbol(hyp, ch, is_space=False)\n",
    "            new_beam.append(cont)\n",
    "\n",
    "            # Вставить пробел перед текущим символом (если допустимо)\n",
    "            can_space = (lwlen >= min_word_len) and (srun < max_consec_spaces)\n",
    "            if can_space:\n",
    "                # Сначала расширяем гипотезу пробелом\n",
    "                sp = extend_with_symbol(hyp, \" \", is_space=True)\n",
    "                lp2, out2, lw2, ctx2, sr2, sp2 = sp\n",
    "                lp2 -= space_penalty\n",
    "                # Добавляем индекс i в список позиций пробелов\n",
    "                sp_after = (lp2, out2, lw2, ctx2, sr2, sp2 + [i])\n",
    "                # После пробела добавляем сам текущий символ\n",
    "                cont2 = extend_with_symbol(sp_after, ch, is_space=False)\n",
    "                new_beam.append(cont2)\n",
    "\n",
    "        # Сортируем новые гипотезы по лог-скорингу (убывание) и обрезаем до ширины луча (beam_size)\n",
    "        new_beam.sort(key = lambda x: x[0], reverse = True)\n",
    "        beam = new_beam[:beam_size]\n",
    "\n",
    "        # Удаляем гипотезы, превысившие лимит длины выходной строки\n",
    "        if any(len(h[1]) > max_len for h in beam):\n",
    "            beam = [h for h in beam if len(h[1]) <= max_len]\n",
    "\n",
    "    # Завершаем последовательность добавлением вероятности EOS к каждой гипотезе\n",
    "    finalized = []\n",
    "    for hyp in beam:\n",
    "        lp, out_chars, lwlen, ctx, srun, spos = hyp\n",
    "        lp_end = lp + lm.logprob_next(ctx, lm.EOS)\n",
    "        finalized.append((lp_end, out_chars, lwlen, ctx, srun, spos))\n",
    "\n",
    "    # Выбираем лучшую финальную гипотезу\n",
    "    finalized.sort(key = lambda x: x[0], reverse = True)\n",
    "    best = finalized[0]\n",
    "    \n",
    "    out_text = \"\".join(best[1])\n",
    "    out_text = \" \".join(out_text.split())\n",
    "    return out_text, best[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9345cb94",
   "metadata": {},
   "source": [
    "Обучение ngramm LM на корпусе очищенных текстов и расстановка пробелов для тестовых данных\n",
    "\n",
    "Подобраны оптимальные параметры для модели и beam search, позволящие достигнуть наилучшего результата\n",
    "\n",
    "results_for_view_data - датасет с восстановленными строками\n",
    "\n",
    "results_data - датасет для тестирования, содержащий только позиции пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb86e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем корпус тренировочных текстов и обучаем LM\n",
    "corpus = load_cleaned_text('processed_text.txt')\n",
    "lm = CharNGramLM(n = 6, add_k = 0.1, lower = True)\n",
    "lm.fit(corpus)\n",
    "\n",
    "# Загружаем тестовые данные и создаем датафремы для сохранения результатов\n",
    "test_text_data = load_test_text('./test_datasets/dataset_1937770_3.txt')\n",
    "\n",
    "results_for_view_data = pd.DataFrame(columns = ['id', 'text_no_spaces', 'text_with_spaces', 'predicted_positions'])\n",
    "results_data = pd.DataFrame(columns = ['id', 'predicted_positions'])\n",
    "\n",
    "# Проходимся по каждой строке и вставлем пробелы с помощью beam search\n",
    "for index, row in test_text_data.iterrows():\n",
    "    id = row['id']\n",
    "    text_no_spaces = row['text_no_spaces']\n",
    "\n",
    "    segmented, space_positions = segment_with_beam(\n",
    "    text = text_no_spaces,\n",
    "    lm = lm,\n",
    "    beam_size = 128,\n",
    "    space_penalty = 0.5,\n",
    "    min_word_len = 1,\n",
    "    max_consec_spaces = 1\n",
    "    )\n",
    "\n",
    "    results_for_view_data.loc[len(results_for_view_data)] = [id, text_no_spaces, segmented, space_positions]\n",
    "    results_data.loc[len(results_data)] = [id, space_positions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd660ab6",
   "metadata": {},
   "source": [
    "Сохранение результатов в файлы csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af00e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_view_data.to_csv('./results/results_for_view_ngram.csv', index = False)\n",
    "results_data.to_csv('./results/results_for_test_ngram.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
